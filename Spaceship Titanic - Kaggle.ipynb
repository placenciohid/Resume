{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Achieving High Accuracy in Predicting Passenger Transportation with Ensemble Models\n",
    "\n",
    "This data is sourced from the Kaggle Competition: [Spaceship Titanic](https://www.kaggle.com/c/spaceship-titanic/overview)\n",
    "\n",
    "# Context\n",
    "\n",
    "\"Welcome to the year 2912, where data science skills are needed to solve a cosmic mystery. We've received a transmission from four lightyears away and things aren't looking good.\n",
    "\n",
    "The Spaceship Titanic was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars.\n",
    "\n",
    "While rounding Alpha Centauri en route to its first destination—the torrid 55 Cancri E—the unwary Spaceship Titanic collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension!\n",
    "\n",
    "To help rescue crews and retrieve the lost passengers, you are challenged to predict which passengers were transported by the anomaly using records recovered from the spaceship’s damaged computer system.\"\n",
    "\n",
    "**Help save them and change history!**\n",
    "\n",
    "# Dataset Description\n",
    "\n",
    "In this competition the task is to predict whether a passenger was transported to an alternate dimension during the Spaceship Titanic's collision with the spacetime anomaly. To help make these predictions, you're given a set of personal records recovered from the ship's damaged computer system.\n",
    "\n",
    "### File and Data Field Descriptions\n",
    "\n",
    "**train.csv** - Personal records for about two-thirds (~8700) of the passengers, to be used as training data.\n",
    "        \n",
    "**Variables**\n",
    "\n",
    "*PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.*\n",
    "\n",
    "*HomePlanet - The planet the passenger departed from, typically their planet of permanent residence.*\n",
    "\n",
    "*CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.*\n",
    "\n",
    "*Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.*\n",
    "\n",
    "*Destination - The planet the passenger will be debarking to.*\n",
    "\n",
    "*Age - The age of the passenger.*\n",
    "\n",
    "*VIP - Whether the passenger has paid for special VIP service during the voyage.*\n",
    "\n",
    "*RoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.*\n",
    "\n",
    "*Name - The first and last names of the passenger.*\n",
    "\n",
    "*Transported - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.*\n",
    "    \n",
    "**test.csv** - Personal records for the remaining one-third (~4300) of the passengers, to be used as test data. Your task is to predict the value of Transported for the passengers in this set.*\n",
    "    \n",
    "**sample_submission.csv** - A submission file in the correct format.\n",
    "\n",
    "*PassengerId - Id for each passenger in the test set.*\n",
    "*Transported - The target. For each passenger, predict either True or False.*\n",
    "\n",
    "# Development\n",
    "\n",
    "In order to achieve high prediction accuracy, different models were tested, fine-tuned, and evaluated based on multiple performance metrics. Below are the models that were used, along with their performance:\n",
    "\n",
    "### CatBoost Classifier:\n",
    "- **Accuracy**: 81.48%\n",
    "- **Precision**: 81.87%\n",
    "- **Recall**: 81.19%\n",
    "- **F1 Score**: 81.53%\n",
    "\n",
    "CatBoost emerged as the best-performing model, effectively handling categorical features and delivering the highest accuracy among all tested models.\n",
    "\n",
    "### XGBoost Classifier:\n",
    "- **Accuracy**: 81.25%\n",
    "- **Precision**: 81.94%\n",
    "- **Recall**: 80.50%\n",
    "- **F1 Score**: 81.21%\n",
    "\n",
    "XGBoost provided robust performance, achieving high precision and recall scores, making it a reliable model in various settings.\n",
    "\n",
    "### Stacking Classifier:\n",
    "- **Accuracy**: 81.02%\n",
    "- **Precision**: 81.17%\n",
    "- **Recall**: 81.11%\n",
    "- **F1 Score**: 81.14%\n",
    "\n",
    "Utilizing a diverse set of models, the Stacking Classifier successfully integrated predictions from CatBoost, XGBoost, LightGBM, Neural Networks, SVM, and Random Forest, demonstrating the power of ensemble learning.\n",
    "\n",
    "### Random Forest Classifier:\n",
    "- **Accuracy**: 80.37%\n",
    "- **Precision**: 81.61%\n",
    "- **Recall**: 78.75%\n",
    "- **F1 Score**: 80.15%\n",
    "\n",
    "The Random Forest model was optimized through hyperparameter tuning and delivered strong precision, indicating its effectiveness for complex datasets.\n",
    "\n",
    "### LightGBM Classifier:\n",
    "- **Accuracy**: 79.91%\n",
    "- **Precision**: 79.91%\n",
    "- **Recall**: 80.27%\n",
    "- **F1 Score**: 80.09%\n",
    "\n",
    "LightGBM provided balanced precision and recall, showcasing its potential in boosting tasks.\n",
    "\n",
    "### Neural Network:\n",
    "- **Accuracy**: 79.72%\n",
    "- **Precision**: 80.39%\n",
    "- **Recall**: 78.98%\n",
    "- **F1 Score**: 79.68%\n",
    "\n",
    "The advanced neural network architecture incorporated residual connections and regularization techniques, yielding competitive results among the ensemble.\n",
    "\n",
    "### Support Vector Machine (SVM):\n",
    "- **Accuracy**: 79.29%\n",
    "- **Precision**: 79.66%\n",
    "- **Recall**: 79.06%\n",
    "- **F1 Score**: 79.36%\n",
    "\n",
    "SVM, with extensive hyperparameter tuning, achieved strong consistency in precision and recall, validating its application for classification tasks.\n",
    "\n",
    "## Final Kaggle Competition Results\n",
    "\n",
    "After fine-tuning and stacking the models, the final submission scored 0.80710 in the Kaggle competition, placing 123rd out of 1,603 participants, ranking in the top 7.7%.\n",
    "\n",
    "The CatBoost and Stacking Classifier models were instrumental in achieving this performance, demonstrating the importance of advanced ensemble methods and careful model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  \\\n",
      "0     0001_01     Europa     False  B/0/P  TRAPPIST-1e  39.0  False   \n",
      "1     0002_01      Earth     False  F/0/S  TRAPPIST-1e  24.0  False   \n",
      "2     0003_01     Europa     False  A/0/S  TRAPPIST-1e  58.0   True   \n",
      "3     0003_02     Europa     False  A/0/S  TRAPPIST-1e  33.0  False   \n",
      "4     0004_01      Earth     False  F/1/S  TRAPPIST-1e  16.0  False   \n",
      "\n",
      "   RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
      "0          0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n",
      "1        109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n",
      "2         43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n",
      "3          0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n",
      "4        303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n",
      "\n",
      "   Transported  \n",
      "0        False  \n",
      "1         True  \n",
      "2        False  \n",
      "3        False  \n",
      "4         True  \n",
      "(8693, 14)\n",
      "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  \\\n",
      "0     0013_01      Earth      True  G/3/S  TRAPPIST-1e  27.0  False   \n",
      "1     0018_01      Earth     False  F/4/S  TRAPPIST-1e  19.0  False   \n",
      "2     0019_01     Europa      True  C/0/S  55 Cancri e  31.0  False   \n",
      "3     0021_01     Europa     False  C/1/S  TRAPPIST-1e  38.0  False   \n",
      "4     0023_01      Earth     False  F/5/S  TRAPPIST-1e  20.0  False   \n",
      "\n",
      "   RoomService  FoodCourt  ShoppingMall     Spa  VRDeck              Name  \n",
      "0          0.0        0.0           0.0     0.0     0.0   Nelly Carsoning  \n",
      "1          0.0        9.0           0.0  2823.0     0.0    Lerome Peckers  \n",
      "2          0.0        0.0           0.0     0.0     0.0   Sabih Unhearfus  \n",
      "3          0.0     6652.0           0.0   181.0   585.0  Meratz Caltilter  \n",
      "4         10.0        0.0         635.0     0.0     0.0   Brence Harperez  \n",
      "(4277, 13)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Activation, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import RFE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load datasets\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Preview the first five rows of the train dataset\n",
    "print(train.head())\n",
    "print(train.shape)\n",
    "\n",
    "# Preview the first five rows of the test dataset\n",
    "print(test.head())\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation - Handling Missing Values\n",
    "def fill_missing_values(df):\n",
    "    numeric_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    categorical_features = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP']\n",
    "    \n",
    "    for feature in numeric_features:\n",
    "        df[feature].fillna(df[feature].mean(), inplace=True)\n",
    "    for feature in categorical_features:\n",
    "        df[feature].fillna(df[feature].mode()[0], inplace=True)\n",
    "    \n",
    "    df['Cabin'].fillna('Z/9999/Z', inplace=True)\n",
    "    df['Name'].fillna('Unknown', inplace=True)\n",
    "    return df\n",
    "\n",
    "train = fill_missing_values(train)\n",
    "test = fill_missing_values(test)\n",
    "\n",
    "# Feature Engineering - Creating New Features\n",
    "def feature_engineering(df):\n",
    "    # Split 'Cabin' into 'Deck', 'CabinNum', 'Side'\n",
    "    df['Deck'] = df['Cabin'].str.split('/').str[0]\n",
    "    df['CabinNum'] = df['Cabin'].str.split('/').str[1].astype(int)\n",
    "    df['Side'] = df['Cabin'].str.split('/').str[2]\n",
    "    df.drop('Cabin', axis=1, inplace=True)\n",
    "    \n",
    "    # Create 'Group' feature from 'PassengerId'\n",
    "    df['Group'] = df['PassengerId'].str.split('_').str[0]\n",
    "    \n",
    "    # Extract 'Surname' from 'Name'\n",
    "    df['Surname'] = df['Name'].str.split().str[-1]\n",
    "    df.drop('Name', axis=1, inplace=True)\n",
    "    \n",
    "    # Total spend\n",
    "    spend_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    df['TotalSpend'] = df[spend_cols].sum(axis=1)\n",
    "    \n",
    "    # Family size\n",
    "    df['FamilySize'] = df.groupby('Group')['Group'].transform('count')\n",
    "    \n",
    "    # Is alone\n",
    "    df['IsAlone'] = np.where(df['FamilySize'] == 1, 1, 0)\n",
    "    \n",
    "    # Drop 'PassengerId' as it's no longer needed\n",
    "    df.drop(['PassengerId', 'Group', 'Surname'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = feature_engineering(train)\n",
    "test = feature_engineering(test)\n",
    "\n",
    "# Convert categorical columns to dummy variables\n",
    "categorical_cols = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Side']\n",
    "train = pd.get_dummies(train, columns=categorical_cols, drop_first=True)\n",
    "test = pd.get_dummies(test, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "y = train['Transported'].astype(int)  # Ensure target is integer\n",
    "X = train.drop('Transported', axis=1)\n",
    "\n",
    "# Align the features of X and test data\n",
    "X, test = X.align(test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "test_scaled = scaler.transform(test)\n",
    "\n",
    "# Feature Selection using Recursive Feature Elimination (RFE)\n",
    "selector = RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=20)\n",
    "selector.fit(X_scaled, y)\n",
    "X_selected = selector.transform(X_scaled)\n",
    "test_selected = selector.transform(test_scaled)\n",
    "selected_features = pd.Series(X.columns[selector.support_])\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_selected, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Define cross-validation strategy\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "Epoch 00046: early stopping\n",
      "Fitting 5 folds for each of 75 candidates, totalling 375 fits\n",
      "            Model  Accuracy  Precision    Recall  F1 Score\n",
      "3        CatBoost  0.814801   0.818740  0.811881  0.815296\n",
      "1         XGBoost  0.812500   0.819380  0.805027  0.812140\n",
      "0   Random Forest  0.803681   0.816101  0.787510  0.801550\n",
      "2        LightGBM  0.799080   0.799090  0.802742  0.800912\n",
      "4  Neural Network  0.797163   0.803876  0.789794  0.796773\n",
      "5             SVM  0.792945   0.796623  0.790556  0.793578\n"
     ]
    }
   ],
   "source": [
    "# Build and Evaluate Models\n",
    "\n",
    "# Initialize a DataFrame to store model performance\n",
    "performance = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "# 1. Random Forest Classifier with Hyperparameter Tuning\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=cv_strategy, n_jobs=-1, verbose=1)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "rf_best = rf_grid.best_estimator_\n",
    "rf_pred = rf_best.predict(X_val)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "performance = performance.append({\n",
    "    'Model': 'Random Forest',\n",
    "    'Accuracy': accuracy_score(y_val, rf_pred),\n",
    "    'Precision': precision_score(y_val, rf_pred),\n",
    "    'Recall': recall_score(y_val, rf_pred),\n",
    "    'F1 Score': f1_score(y_val, rf_pred)\n",
    "}, ignore_index=True)\n",
    "\n",
    "# 2. XGBoost Classifier with Hyperparameter Tuning\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 1.0]\n",
    "}\n",
    "xgb_grid = GridSearchCV(XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'), xgb_params, cv=cv_strategy, n_jobs=-1, verbose=1)\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "xgb_best = xgb_grid.best_estimator_\n",
    "xgb_pred = xgb_best.predict(X_val)\n",
    "\n",
    "# Evaluate XGBoost\n",
    "performance = performance.append({\n",
    "    'Model': 'XGBoost',\n",
    "    'Accuracy': accuracy_score(y_val, xgb_pred),\n",
    "    'Precision': precision_score(y_val, xgb_pred),\n",
    "    'Recall': recall_score(y_val, xgb_pred),\n",
    "    'F1 Score': f1_score(y_val, xgb_pred)\n",
    "}, ignore_index=True)\n",
    "\n",
    "# 3. LightGBM Classifier with Hyperparameter Tuning\n",
    "lgb_params = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'num_leaves': [31, 63],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 1.0]\n",
    "}\n",
    "lgb_grid = GridSearchCV(LGBMClassifier(random_state=42), lgb_params, cv=cv_strategy, n_jobs=-1, verbose=1)\n",
    "lgb_grid.fit(X_train, y_train)\n",
    "lgb_best = lgb_grid.best_estimator_\n",
    "lgb_pred = lgb_best.predict(X_val)\n",
    "\n",
    "# Evaluate LightGBM\n",
    "performance = performance.append({\n",
    "    'Model': 'LightGBM',\n",
    "    'Accuracy': accuracy_score(y_val, lgb_pred),\n",
    "    'Precision': precision_score(y_val, lgb_pred),\n",
    "    'Recall': recall_score(y_val, lgb_pred),\n",
    "    'F1 Score': f1_score(y_val, lgb_pred)\n",
    "}, ignore_index=True)\n",
    "\n",
    "# 4. CatBoost Classifier\n",
    "cat_model = CatBoostClassifier(verbose=0, random_state=42)\n",
    "cat_model.fit(X_train, y_train)\n",
    "cat_pred = cat_model.predict(X_val)\n",
    "\n",
    "# Evaluate CatBoost\n",
    "performance = performance.append({\n",
    "    'Model': 'CatBoost',\n",
    "    'Accuracy': accuracy_score(y_val, cat_pred),\n",
    "    'Precision': precision_score(y_val, cat_pred),\n",
    "    'Recall': recall_score(y_val, cat_pred),\n",
    "    'F1 Score': f1_score(y_val, cat_pred)\n",
    "}, ignore_index=True)\n",
    "\n",
    "# 5. Deep Neural Network with Advanced Architecture using KerasClassifier\n",
    "\n",
    "# Custom KerasClassifier to handle the absence of predict_classes\n",
    "class MyKerasClassifier(KerasClassifier):\n",
    "    _estimator_type = \"classifier\"  # Ensure scikit-learn recognizes it as a classifier\n",
    "\n",
    "    def predict(self, x, **kwargs):\n",
    "        \"\"\"Override the default predict method to handle predict_classes absence.\"\"\"\n",
    "        proba = self.model.predict(x)\n",
    "        if proba.shape[-1] > 1:\n",
    "            return proba.argmax(axis=-1)\n",
    "        else:\n",
    "            return (proba > 0.5).astype(\"int32\")\n",
    "\n",
    "    def predict_proba(self, x, **kwargs):\n",
    "        \"\"\"Return class probabilities.\"\"\"\n",
    "        proba = self.model.predict(x)\n",
    "        if proba.shape[-1] == 1:\n",
    "            # Binary classification\n",
    "            return np.hstack([1 - proba, proba])\n",
    "        else:\n",
    "            # Multi-class classification\n",
    "            return proba\n",
    "\n",
    "def build_advanced_nn():\n",
    "    inputs = Input(shape=(X_train.shape[1],))\n",
    "    \n",
    "    # Encoder\n",
    "    x = Dense(512, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Residual Block\n",
    "    shortcut = x\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1, min_lr=1e-6)\n",
    "\n",
    "# Wrap the model using MyKerasClassifier\n",
    "nn_model_wrapped = MyKerasClassifier(\n",
    "    build_fn=build_advanced_nn,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Fit the wrapped neural network\n",
    "nn_model_wrapped.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "nn_pred = nn_model_wrapped.predict(X_val)\n",
    "\n",
    "# Evaluate Neural Network\n",
    "performance = performance.append({\n",
    "    'Model': 'Neural Network',\n",
    "    'Accuracy': accuracy_score(y_val, nn_pred),\n",
    "    'Precision': precision_score(y_val, nn_pred),\n",
    "    'Recall': recall_score(y_val, nn_pred),\n",
    "    'F1 Score': f1_score(y_val, nn_pred)\n",
    "}, ignore_index=True)\n",
    "\n",
    "# 6. Support Vector Machine with Extensive Hyperparameter Tuning\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'gamma': ['scale', 'auto', 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "svm_grid = GridSearchCV(SVC(probability=True, random_state=42), svm_params, cv=cv_strategy, n_jobs=-1, verbose=1)\n",
    "svm_grid.fit(X_train, y_train)\n",
    "svm_best = svm_grid.best_estimator_\n",
    "svm_pred = svm_best.predict(X_val)\n",
    "\n",
    "# Evaluate SVM\n",
    "performance = performance.append({\n",
    "    'Model': 'SVM',\n",
    "    'Accuracy': accuracy_score(y_val, svm_pred),\n",
    "    'Precision': precision_score(y_val, svm_pred),\n",
    "    'Recall': recall_score(y_val, svm_pred),\n",
    "    'F1 Score': f1_score(y_val, svm_pred)\n",
    "}, ignore_index=True)\n",
    "\n",
    "# Display performance\n",
    "print(performance.sort_values(by='Accuracy', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy  Precision    Recall  F1 Score\n",
      "3             CatBoost  0.814801   0.818740  0.811881  0.815296\n",
      "1              XGBoost  0.812500   0.819380  0.805027  0.812140\n",
      "6  Stacking Classifier  0.810199   0.811738  0.811120  0.811429\n",
      "0        Random Forest  0.803681   0.816101  0.787510  0.801550\n",
      "2             LightGBM  0.799080   0.799090  0.802742  0.800912\n",
      "4       Neural Network  0.797163   0.803876  0.789794  0.796773\n",
      "5                  SVM  0.792945   0.796623  0.790556  0.793578\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Techniques - Stacking Classifier\n",
    "estimators = [\n",
    "    ('rf', rf_best),\n",
    "    ('xgb', xgb_best),\n",
    "    ('lgb', lgb_best),\n",
    "    ('cat', cat_model),\n",
    "    ('svm', svm_best),\n",
    "    ('nn', nn_model_wrapped)\n",
    "]\n",
    "\n",
    "# Meta-classifier\n",
    "meta_classifier = LogisticRegression()\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=meta_classifier,\n",
    "    cv=cv_strategy,\n",
    "    n_jobs=1,  # Set n_jobs=1 to avoid pickling issues\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "# Fit the stacking classifier\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "stacking_pred = stacking_clf.predict(X_val)\n",
    "\n",
    "# Evaluate Stacking Classifier\n",
    "performance = performance.append({\n",
    "    'Model': 'Stacking Classifier',\n",
    "    'Accuracy': accuracy_score(y_val, stacking_pred),\n",
    "    'Precision': precision_score(y_val, stacking_pred),\n",
    "    'Recall': recall_score(y_val, stacking_pred),\n",
    "    'F1 Score': f1_score(y_val, stacking_pred)\n",
    "}, ignore_index=True)\n",
    "\n",
    "# Display performance\n",
    "print(performance.sort_values(by='Accuracy', ascending=False))\n",
    "\n",
    "# Final Model Selection\n",
    "final_model = stacking_clf\n",
    "final_model.fit(X_selected, y)\n",
    "\n",
    "# Make predictions on test data\n",
    "final_predictions = final_model.predict(test_selected)\n",
    "\n",
    "# Prepare submission file\n",
    "submission = pd.read_csv('test.csv')[['PassengerId']]\n",
    "submission['Transported'] = final_predictions.astype(bool)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "E023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
